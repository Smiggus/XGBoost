{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import databento_sqlmod1 as dbs\n",
    "#import databento_sql as dbs\n",
    "import fmp_keymetrics\n",
    "import combiner\n",
    "import numpy as np\n",
    "#import talib as ta  # ta-lib or ta for calculating technical indicators\n",
    "import ta\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = ['AAPL', 'MSFT', 'NVDA',  # Technology\n",
    "    'AMZN', 'TSLA', 'HD',  # Consumer Discretionary\n",
    "    'UNH', 'JNJ', 'LLY',  # Healthcare\n",
    "    'JPM', 'BAC', 'WFC',  # Financials\n",
    "    'XOM', 'CVX', 'COP',  # Energy\n",
    "    'PG', 'KO', 'PEP',  # Consumer Staples\n",
    "    'BA', 'CAT', 'UPS',  # Industrials\n",
    "    'LIN', 'APD', 'SHW',  # Materials\n",
    "    'PLD', 'AMT', 'CCI',  # Real Estate\n",
    "    'NEE', 'DUK', 'SO',  # Utilities\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old prepdata function\n",
    "'''\n",
    "def prepdata(ticker, start_date, end_date):\n",
    "    # Retrieve data for the ticker and date range\n",
    "    data = comb.get_comb_data_from_postgresql(ticker, start_date, end_date)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Sort the data by date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(by='date')\n",
    "\n",
    "    # Add technical indicators\n",
    "    df = add_technical_indicators(df)\n",
    "\n",
    "    # Calculate log returns as the target variable\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "    # Drop rows with NaN values in log returns and technical indicators\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Define the fundamental columns and technical indicators to use as features\n",
    "    fundamental_columns = [\n",
    "        'revenuePerShare', 'netIncomePerShare', 'operatingCashFlowPerShare', 'freeCashFlowPerShare', \n",
    "        'cashPerShare', 'bookValuePerShare', 'tangibleBookValuePerShare', 'shareholdersEquityPerShare', \n",
    "        'interestDebtPerShare', 'marketCap', 'enterpriseValue', 'peRatio', 'priceToSalesRatio', \n",
    "        'pocfratio', 'pfcfRatio', 'pbRatio', 'ptbRatio', 'evToSales', 'enterpriseValueOverEBITDA', \n",
    "        'evToOperatingCashFlow', 'evToFreeCashFlow', 'earningsYield', 'freeCashFlowYield', \n",
    "        'debtToEquity', 'debtToAssets', 'netDebtToEBITDA', 'currentRatio', 'interestCoverage', \n",
    "        'incomeQuality', 'dividendYield', 'payoutRatio', 'salesGeneralAndAdministrativeToRevenue', \n",
    "        'researchAndDdevelopementToRevenue', 'intangiblesToTotalAssets', 'capexToOperatingCashFlow', \n",
    "        'capexToRevenue', 'capexToDepreciation', 'stockBasedCompensationToRevenue', 'grahamNumber', \n",
    "        'roic', 'returnOnTangibleAssets', 'grahamNetNet', 'workingCapital', 'tangibleAssetValue', \n",
    "        'netCurrentAssetValue', 'investedCapital', 'averageReceivables', 'averagePayables', \n",
    "        'averageInventory', 'daysSalesOutstanding', 'daysPayablesOutstanding', \n",
    "        'daysOfInventoryOnHand', 'receivablesTurnover', 'payablesTurnover', 'inventoryTurnover', \n",
    "        'roe', 'capexPerShare'\n",
    "    ]\n",
    "    \n",
    "    # Technical columns added\n",
    "    technical_columns = [\n",
    "        'sma_20', 'sma_50', 'ema_20', 'rsi_14', 'macd', 'macd_signal', 'bb_upper', 'bb_lower', \n",
    "        'atr_14', 'obv', 'stoch_k', 'stoch_d', 'williams_r', 'cmf', 'ema_50', 'ema_100', 'ema_200'\n",
    "    ]\n",
    "\n",
    "    # Fill missing values in the fundamental columns using forward fill, followed by zeros if necessary\n",
    "    df[fundamental_columns + technical_columns] = df[fundamental_columns + technical_columns].ffill().fillna(0)\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    X = df[fundamental_columns + technical_columns]\n",
    "    y = df['log_return']\n",
    "\n",
    "    # Convert features into an XGBoost DMatrix\n",
    "    dmat = xgb.DMatrix(X, label=y, missing=np.nan)\n",
    "\n",
    "    return dmat, y, X'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker AAPL already up-to-date\n",
      "data/equity/usa/daily/aapl.csv has been successfully zipped into data/equity/usa/daily/aapl.zip.\n",
      "Data for AAPL fetched from postgres, converted to LEAN format.\n",
      "Ticker MSFT already up-to-date\n",
      "data/equity/usa/daily/msft.csv has been successfully zipped into data/equity/usa/daily/msft.zip.\n",
      "Data for MSFT fetched from postgres, converted to LEAN format.\n",
      "Ticker NVDA already up-to-date\n",
      "data/equity/usa/daily/nvda.csv has been successfully zipped into data/equity/usa/daily/nvda.zip.\n",
      "Data for NVDA fetched from postgres, converted to LEAN format.\n",
      "Ticker AMZN already up-to-date\n",
      "data/equity/usa/daily/amzn.csv has been successfully zipped into data/equity/usa/daily/amzn.zip.\n",
      "Data for AMZN fetched from postgres, converted to LEAN format.\n",
      "Ticker TSLA already up-to-date\n",
      "data/equity/usa/daily/tsla.csv has been successfully zipped into data/equity/usa/daily/tsla.zip.\n",
      "Data for TSLA fetched from postgres, converted to LEAN format.\n",
      "Ticker HD already up-to-date\n",
      "data/equity/usa/daily/hd.csv has been successfully zipped into data/equity/usa/daily/hd.zip.\n",
      "Data for HD fetched from postgres, converted to LEAN format.\n",
      "Ticker UNH already up-to-date\n",
      "data/equity/usa/daily/unh.csv has been successfully zipped into data/equity/usa/daily/unh.zip.\n",
      "Data for UNH fetched from postgres, converted to LEAN format.\n",
      "Ticker JNJ already up-to-date\n",
      "data/equity/usa/daily/jnj.csv has been successfully zipped into data/equity/usa/daily/jnj.zip.\n",
      "Data for JNJ fetched from postgres, converted to LEAN format.\n",
      "Ticker LLY already up-to-date\n",
      "data/equity/usa/daily/lly.csv has been successfully zipped into data/equity/usa/daily/lly.zip.\n",
      "Data for LLY fetched from postgres, converted to LEAN format.\n",
      "Ticker JPM already up-to-date\n",
      "data/equity/usa/daily/jpm.csv has been successfully zipped into data/equity/usa/daily/jpm.zip.\n",
      "Data for JPM fetched from postgres, converted to LEAN format.\n",
      "Ticker BAC already up-to-date\n",
      "data/equity/usa/daily/bac.csv has been successfully zipped into data/equity/usa/daily/bac.zip.\n",
      "Data for BAC fetched from postgres, converted to LEAN format.\n",
      "Ticker WFC already up-to-date\n",
      "data/equity/usa/daily/wfc.csv has been successfully zipped into data/equity/usa/daily/wfc.zip.\n",
      "Data for WFC fetched from postgres, converted to LEAN format.\n",
      "Ticker XOM already up-to-date\n",
      "data/equity/usa/daily/xom.csv has been successfully zipped into data/equity/usa/daily/xom.zip.\n",
      "Data for XOM fetched from postgres, converted to LEAN format.\n",
      "Ticker CVX already up-to-date\n",
      "data/equity/usa/daily/cvx.csv has been successfully zipped into data/equity/usa/daily/cvx.zip.\n",
      "Data for CVX fetched from postgres, converted to LEAN format.\n",
      "Ticker COP already up-to-date\n",
      "data/equity/usa/daily/cop.csv has been successfully zipped into data/equity/usa/daily/cop.zip.\n",
      "Data for COP fetched from postgres, converted to LEAN format.\n",
      "Ticker PG already up-to-date\n",
      "data/equity/usa/daily/pg.csv has been successfully zipped into data/equity/usa/daily/pg.zip.\n",
      "Data for PG fetched from postgres, converted to LEAN format.\n",
      "Ticker KO already up-to-date\n",
      "data/equity/usa/daily/ko.csv has been successfully zipped into data/equity/usa/daily/ko.zip.\n",
      "Data for KO fetched from postgres, converted to LEAN format.\n",
      "Ticker PEP already up-to-date\n",
      "data/equity/usa/daily/pep.csv has been successfully zipped into data/equity/usa/daily/pep.zip.\n",
      "Data for PEP fetched from postgres, converted to LEAN format.\n",
      "Ticker BA already up-to-date\n",
      "data/equity/usa/daily/ba.csv has been successfully zipped into data/equity/usa/daily/ba.zip.\n",
      "Data for BA fetched from postgres, converted to LEAN format.\n",
      "Ticker CAT already up-to-date\n",
      "data/equity/usa/daily/cat.csv has been successfully zipped into data/equity/usa/daily/cat.zip.\n",
      "Data for CAT fetched from postgres, converted to LEAN format.\n",
      "Ticker UPS already up-to-date\n",
      "data/equity/usa/daily/ups.csv has been successfully zipped into data/equity/usa/daily/ups.zip.\n",
      "Data for UPS fetched from postgres, converted to LEAN format.\n",
      "Ticker LIN already up-to-date\n",
      "data/equity/usa/daily/lin.csv has been successfully zipped into data/equity/usa/daily/lin.zip.\n",
      "Data for LIN fetched from postgres, converted to LEAN format.\n",
      "Ticker APD already up-to-date\n",
      "data/equity/usa/daily/apd.csv has been successfully zipped into data/equity/usa/daily/apd.zip.\n",
      "Data for APD fetched from postgres, converted to LEAN format.\n",
      "Ticker SHW already up-to-date\n",
      "data/equity/usa/daily/shw.csv has been successfully zipped into data/equity/usa/daily/shw.zip.\n",
      "Data for SHW fetched from postgres, converted to LEAN format.\n",
      "Ticker PLD already up-to-date\n",
      "data/equity/usa/daily/pld.csv has been successfully zipped into data/equity/usa/daily/pld.zip.\n",
      "Data for PLD fetched from postgres, converted to LEAN format.\n",
      "Ticker AMT already up-to-date\n",
      "data/equity/usa/daily/amt.csv has been successfully zipped into data/equity/usa/daily/amt.zip.\n",
      "Data for AMT fetched from postgres, converted to LEAN format.\n",
      "Ticker CCI already up-to-date\n",
      "data/equity/usa/daily/cci.csv has been successfully zipped into data/equity/usa/daily/cci.zip.\n",
      "Data for CCI fetched from postgres, converted to LEAN format.\n",
      "Ticker NEE already up-to-date\n",
      "data/equity/usa/daily/nee.csv has been successfully zipped into data/equity/usa/daily/nee.zip.\n",
      "Data for NEE fetched from postgres, converted to LEAN format.\n",
      "Ticker DUK already up-to-date\n",
      "data/equity/usa/daily/duk.csv has been successfully zipped into data/equity/usa/daily/duk.zip.\n",
      "Data for DUK fetched from postgres, converted to LEAN format.\n",
      "Ticker SO already up-to-date\n",
      "data/equity/usa/daily/so.csv has been successfully zipped into data/equity/usa/daily/so.zip.\n",
      "Data for SO fetched from postgres, converted to LEAN format.\n",
      "Data for AAPL already exists in the database for the requested range.\n",
      "Data for MSFT already exists in the database for the requested range.\n",
      "Data for NVDA already exists in the database for the requested range.\n",
      "Data for AMZN already exists in the database for the requested range.\n",
      "Data for TSLA already exists in the database for the requested range.\n",
      "Data for HD already exists in the database for the requested range.\n",
      "Data for UNH already exists in the database for the requested range.\n",
      "Data for JNJ already exists in the database for the requested range.\n",
      "Data for LLY already exists in the database for the requested range.\n",
      "Data for JPM already exists in the database for the requested range.\n",
      "Data for BAC already exists in the database for the requested range.\n",
      "Data for WFC already exists in the database for the requested range.\n",
      "Data for XOM already exists in the database for the requested range.\n",
      "Data for CVX already exists in the database for the requested range.\n",
      "Data for COP already exists in the database for the requested range.\n",
      "Data for PG already exists in the database for the requested range.\n",
      "Data for KO already exists in the database for the requested range.\n",
      "Data for PEP already exists in the database for the requested range.\n",
      "Data for BA already exists in the database for the requested range.\n",
      "Data for CAT already exists in the database for the requested range.\n",
      "Data for UPS already exists in the database for the requested range.\n",
      "Data for LIN already exists in the database for the requested range.\n",
      "Data for APD already exists in the database for the requested range.\n",
      "Data for SHW already exists in the database for the requested range.\n",
      "Data for PLD already exists in the database for the requested range.\n",
      "Data for AMT already exists in the database for the requested range.\n",
      "Data for CCI already exists in the database for the requested range.\n",
      "Data for NEE already exists in the database for the requested range.\n",
      "Data for DUK already exists in the database for the requested range.\n",
      "Data for SO already exists in the database for the requested range.\n",
      "No data fetched for any of the stocks.\n",
      "PriceVol data loaded for AAPL\n",
      "Fundamental data loaded for AAPL\n",
      "Price-volume fundamental data for AAPL saved to table comb_pvol_funda.AAPL\n",
      "PriceVol data loaded for MSFT\n",
      "Fundamental data loaded for MSFT\n",
      "Price-volume fundamental data for MSFT saved to table comb_pvol_funda.MSFT\n",
      "PriceVol data loaded for NVDA\n",
      "Fundamental data loaded for NVDA\n",
      "Price-volume fundamental data for NVDA saved to table comb_pvol_funda.NVDA\n",
      "PriceVol data loaded for AMZN\n",
      "Fundamental data loaded for AMZN\n",
      "Price-volume fundamental data for AMZN saved to table comb_pvol_funda.AMZN\n",
      "PriceVol data loaded for TSLA\n",
      "Fundamental data loaded for TSLA\n",
      "Price-volume fundamental data for TSLA saved to table comb_pvol_funda.TSLA\n",
      "PriceVol data loaded for HD\n",
      "Fundamental data loaded for HD\n",
      "Price-volume fundamental data for HD saved to table comb_pvol_funda.HD\n",
      "PriceVol data loaded for UNH\n",
      "Fundamental data loaded for UNH\n",
      "Price-volume fundamental data for UNH saved to table comb_pvol_funda.UNH\n",
      "PriceVol data loaded for JNJ\n",
      "Fundamental data loaded for JNJ\n",
      "Price-volume fundamental data for JNJ saved to table comb_pvol_funda.JNJ\n",
      "PriceVol data loaded for LLY\n",
      "Fundamental data loaded for LLY\n",
      "Price-volume fundamental data for LLY saved to table comb_pvol_funda.LLY\n",
      "PriceVol data loaded for JPM\n",
      "Fundamental data loaded for JPM\n",
      "Price-volume fundamental data for JPM saved to table comb_pvol_funda.JPM\n",
      "PriceVol data loaded for BAC\n",
      "Fundamental data loaded for BAC\n",
      "Price-volume fundamental data for BAC saved to table comb_pvol_funda.BAC\n",
      "PriceVol data loaded for WFC\n",
      "Fundamental data loaded for WFC\n",
      "Price-volume fundamental data for WFC saved to table comb_pvol_funda.WFC\n",
      "PriceVol data loaded for XOM\n",
      "Fundamental data loaded for XOM\n",
      "Price-volume fundamental data for XOM saved to table comb_pvol_funda.XOM\n",
      "PriceVol data loaded for CVX\n",
      "Fundamental data loaded for CVX\n",
      "Price-volume fundamental data for CVX saved to table comb_pvol_funda.CVX\n",
      "PriceVol data loaded for COP\n",
      "Fundamental data loaded for COP\n",
      "Price-volume fundamental data for COP saved to table comb_pvol_funda.COP\n",
      "PriceVol data loaded for PG\n",
      "Fundamental data loaded for PG\n",
      "Price-volume fundamental data for PG saved to table comb_pvol_funda.PG\n",
      "PriceVol data loaded for KO\n",
      "Fundamental data loaded for KO\n",
      "Price-volume fundamental data for KO saved to table comb_pvol_funda.KO\n",
      "PriceVol data loaded for PEP\n",
      "Fundamental data loaded for PEP\n",
      "Price-volume fundamental data for PEP saved to table comb_pvol_funda.PEP\n",
      "PriceVol data loaded for BA\n",
      "Fundamental data loaded for BA\n",
      "Price-volume fundamental data for BA saved to table comb_pvol_funda.BA\n",
      "PriceVol data loaded for CAT\n",
      "Fundamental data loaded for CAT\n",
      "Price-volume fundamental data for CAT saved to table comb_pvol_funda.CAT\n",
      "PriceVol data loaded for UPS\n",
      "Fundamental data loaded for UPS\n",
      "Price-volume fundamental data for UPS saved to table comb_pvol_funda.UPS\n",
      "PriceVol data loaded for LIN\n",
      "Fundamental data loaded for LIN\n",
      "Price-volume fundamental data for LIN saved to table comb_pvol_funda.LIN\n",
      "PriceVol data loaded for APD\n",
      "Fundamental data loaded for APD\n",
      "Price-volume fundamental data for APD saved to table comb_pvol_funda.APD\n",
      "PriceVol data loaded for SHW\n",
      "Fundamental data loaded for SHW\n",
      "Price-volume fundamental data for SHW saved to table comb_pvol_funda.SHW\n",
      "PriceVol data loaded for PLD\n",
      "Fundamental data loaded for PLD\n",
      "Price-volume fundamental data for PLD saved to table comb_pvol_funda.PLD\n",
      "PriceVol data loaded for AMT\n",
      "Fundamental data loaded for AMT\n",
      "Price-volume fundamental data for AMT saved to table comb_pvol_funda.AMT\n",
      "PriceVol data loaded for CCI\n",
      "Fundamental data loaded for CCI\n",
      "Price-volume fundamental data for CCI saved to table comb_pvol_funda.CCI\n",
      "PriceVol data loaded for NEE\n",
      "Fundamental data loaded for NEE\n",
      "Price-volume fundamental data for NEE saved to table comb_pvol_funda.NEE\n",
      "PriceVol data loaded for DUK\n",
      "Fundamental data loaded for DUK\n",
      "Price-volume fundamental data for DUK saved to table comb_pvol_funda.DUK\n",
      "PriceVol data loaded for SO\n",
      "Fundamental data loaded for SO\n",
      "Price-volume fundamental data for SO saved to table comb_pvol_funda.SO\n"
     ]
    }
   ],
   "source": [
    "# Databento download\n",
    "for ticker in ticker_list:\n",
    "    dbs.download_and_append_data(ticker, '2019-01-01', '2023-12-31', frequency='daily')\n",
    "\n",
    "#FMP Download\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2024-09-31'\n",
    "\n",
    "fmp = fmp_keymetrics.funda_ETL()\n",
    "keymetrics = fmp.download_funda_data(ticker_list, start=start_date, end=end_date)\n",
    "keymetrics\n",
    "\n",
    "# Merge Databento and FMP data\n",
    "comb = combiner.combine_pvol_funda()\n",
    "#tickers = ['AAPL','AMZN','GOOGL','MSFT']\n",
    "merged = comb.merge_pvol_funda(ticker_list)\n",
    "\n",
    "# Initialize combiner instance\n",
    "comb = combiner.combine_pvol_funda()\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    # Add SMA, EMA\n",
    "    df['sma_20'] = ta.trend.sma_indicator(df['close'], window=20)\n",
    "    df['sma_50'] = ta.trend.sma_indicator(df['close'], window=50)\n",
    "    df['ema_20'] = ta.trend.ema_indicator(df['close'], window=20)\n",
    "    df['ema_50'] = ta.trend.ema_indicator(df['close'], window=50)\n",
    "    df['ema_100'] = ta.trend.ema_indicator(df['close'], window=100)\n",
    "    df['ema_200'] = ta.trend.ema_indicator(df['close'], window=200)\n",
    "    \n",
    "    # Add RSI, MACD\n",
    "    df['rsi_14'] = ta.momentum.rsi(df['close'], window=14)\n",
    "    df['macd'] = ta.trend.macd(df['close'])\n",
    "    df['macd_signal'] = ta.trend.macd_signal(df['close'])\n",
    "\n",
    "    # Add Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(df['close'], window=20)\n",
    "    df['bb_upper'] = bollinger.bollinger_hband()\n",
    "    df['bb_lower'] = bollinger.bollinger_lband()\n",
    "    \n",
    "    # Add Average True Range (ATR)\n",
    "    df['atr_14'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'], window=14)\n",
    "    \n",
    "    # Add On-Balance Volume (OBV)\n",
    "    df['obv'] = ta.volume.on_balance_volume(df['close'], df['volume'])\n",
    "    \n",
    "    # Add Stochastic Oscillator (K, D)\n",
    "    stoch = ta.momentum.stoch(df['high'], df['low'], df['close'], window=14, smooth_window=3)\n",
    "    df['stoch_k'] = stoch\n",
    "    df['stoch_d'] = ta.momentum.stoch_signal(df['high'], df['low'], df['close'], window=14, smooth_window=3)\n",
    "    \n",
    "    # Remove momentum since it was causing an error\n",
    "    # Add Williams %R\n",
    "    df['williams_r'] = ta.momentum.williams_r(df['high'], df['low'], df['close'], lbp=14)\n",
    "    \n",
    "    # Add Chaikin Money Flow (CMF)\n",
    "    df['cmf'] = ta.volume.chaikin_money_flow(df['high'], df['low'], df['close'], df['volume'], window=20)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepdata(ticker, start_date, end_date):\n",
    "    # Retrieve data for the ticker and date range\n",
    "    data = comb.get_comb_data_from_postgresql(ticker, start_date, end_date)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Sort the data by date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(by='date')\n",
    "\n",
    "    # Define the fundamental columns and technical indicators to use as features\n",
    "    fundamental_columns = [\n",
    "        'revenuePerShare', 'netIncomePerShare', 'operatingCashFlowPerShare', 'freeCashFlowPerShare', \n",
    "        'cashPerShare', 'bookValuePerShare', 'tangibleBookValuePerShare', 'shareholdersEquityPerShare', \n",
    "        'interestDebtPerShare', 'marketCap', 'enterpriseValue', 'peRatio', 'priceToSalesRatio', \n",
    "        'pocfratio', 'pfcfRatio', 'pbRatio', 'ptbRatio', 'evToSales', 'enterpriseValueOverEBITDA', \n",
    "        'evToOperatingCashFlow', 'evToFreeCashFlow', 'earningsYield', 'freeCashFlowYield', \n",
    "        'debtToEquity', 'debtToAssets', 'netDebtToEBITDA', 'currentRatio', 'interestCoverage', \n",
    "        'incomeQuality', 'dividendYield', 'payoutRatio', 'salesGeneralAndAdministrativeToRevenue', \n",
    "        'researchAndDdevelopementToRevenue', 'intangiblesToTotalAssets', 'capexToOperatingCashFlow', \n",
    "        'capexToRevenue', 'capexToDepreciation', 'stockBasedCompensationToRevenue', 'grahamNumber', \n",
    "        'roic', 'returnOnTangibleAssets', 'grahamNetNet', 'workingCapital', 'tangibleAssetValue', \n",
    "        'netCurrentAssetValue', 'investedCapital', 'averageReceivables', 'averagePayables', \n",
    "        'averageInventory', 'daysSalesOutstanding', 'daysPayablesOutstanding', \n",
    "        'daysOfInventoryOnHand', 'receivablesTurnover', 'payablesTurnover', 'inventoryTurnover', \n",
    "        'roe', 'capexPerShare'\n",
    "    ]\n",
    "\n",
    "    # Technical columns added\n",
    "    technical_columns = [\n",
    "        'sma_20', 'sma_50', 'ema_20', 'rsi_14', 'macd', 'macd_signal', 'bb_upper', 'bb_lower', \n",
    "        'atr_14', 'obv', 'stoch_k', 'stoch_d', 'williams_r', 'cmf', 'ema_50', 'ema_100', 'ema_200'\n",
    "    ]\n",
    "\n",
    "    # Add technical indicators\n",
    "    df = add_technical_indicators(df)\n",
    "\n",
    "    # Calculate log returns as the target variable\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "    # Fill missing values in the fundamental columns using forward fill, followed by backward fill, then zeros if necessary\n",
    "    df[fundamental_columns + technical_columns] = df[fundamental_columns + technical_columns].ffill().bfill().fillna(0)\n",
    "\n",
    "    # Drop rows with NaN values in log returns and technical indicators\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    X = df[fundamental_columns + technical_columns]\n",
    "    y = df['log_return']\n",
    "\n",
    "    # Convert features into an XGBoost DMatrix\n",
    "    dmat = xgb.DMatrix(X, label=y, missing=np.nan)\n",
    "\n",
    "    return dmat, y, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers:   0%|          | 0/30 [00:00<?, ?ticker/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 30/30 [00:08<00:00,  3.74ticker/s]\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use the updated function\n",
    "#ticker_list = ['AAPL']\n",
    "for ticker in tqdm(ticker_list, desc=\"Processing tickers\", unit=\"ticker\"):\n",
    "    start_date = '2019-01-01'\n",
    "    end_date = '2022-12-31'\n",
    "\n",
    "    # Prepare the training data using log returns\n",
    "    dtrain, y_train, X_train = prepdata(ticker, start_date, end_date)\n",
    "\n",
    "    ''' Setting Test Data '''\n",
    "    start_date = '2023-01-01'\n",
    "    end_date = '2023-12-31'\n",
    "\n",
    "    # Prepare the test data\n",
    "    dtest, y_test, X_test = prepdata(ticker, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_trade(trade_log, ticker, date, position_type, trade_type, entry_price, exit_price=None, num_shares=None, signal=None, portfolio_value_start=None, portfolio_value_end=None):\n",
    "    \"\"\"\n",
    "    Log trade details into the trade_log DataFrame.\n",
    "    \"\"\"\n",
    "    return_on_trade = None\n",
    "    if exit_price is not None and entry_price is not None:\n",
    "        return_on_trade = (exit_price - entry_price) / entry_price\n",
    "\n",
    "    trade_entry = pd.DataFrame({\n",
    "        'ticker': [ticker],\n",
    "        'date': [date],\n",
    "        'position_type': [position_type],  # 'long' or 'short'\n",
    "        'trade_type': [trade_type],  # 'buy' or 'sell'\n",
    "        'entry_price': [entry_price],\n",
    "        'exit_price': [exit_price],\n",
    "        'return': [return_on_trade],\n",
    "        'num_shares': [num_shares],  # Track the number of shares bought/sold\n",
    "        'signal': [signal],\n",
    "        'portfolio_value_start': [portfolio_value_start],  # Log portfolio value at start of trade\n",
    "        'portfolio_value_end': [portfolio_value_end]  # Log portfolio value at end of trade\n",
    "    })\n",
    "\n",
    "    trade_log = pd.concat([trade_log, trade_entry], ignore_index=True)\n",
    "    \n",
    "    return trade_log\n",
    "\n",
    "def calculate_portfolio_metrics(trade_log_df):\n",
    "    \"\"\"\n",
    "    Calculate portfolio metrics based on the trade log.\n",
    "    \"\"\"\n",
    "    if trade_log_df['return'].empty or trade_log_df['return'].isna().all():\n",
    "        return {'Sharpe Ratio': float('nan'), 'Sortino Ratio': float('nan'), 'Max Drawdown': float('nan')}\n",
    "    \n",
    "    trade_returns = trade_log_df['return'].dropna()\n",
    "    sharpe_ratio = (trade_returns.mean() / trade_returns.std()) * np.sqrt(252) if trade_returns.std() > 0 else float('nan')\n",
    "    downside_risk = trade_returns[trade_returns < 0].std()\n",
    "    sortino_ratio = (trade_returns.mean() / downside_risk) * np.sqrt(252) if downside_risk > 0 else float('nan')\n",
    "\n",
    "    cumulative_return = (1 + trade_returns).cumprod()\n",
    "    drawdown = cumulative_return / cumulative_return.cummax() - 1\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    return {\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Sortino Ratio': sortino_ratio,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "Backtester"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (250, 11), X_test shape: (250, 74), y_test shape: (250,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (250, 11), X_test shape: (250, 74), y_test shape: (250,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n",
      "Test data shape: (249, 11), X_test shape: (249, 74), y_test shape: (249,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4975/230901958.py:140: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  portfolio_value_df = pd.concat([portfolio_value_df, pd.DataFrame({'date': [current_date], 'portfolio_value': [portfolio_value], 'cash_value': [cash], 'position_value': [position_value]})], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Portfolio Value: 100000\n",
      "{'Sharpe Ratio': nan, 'Sortino Ratio': nan, 'Max Drawdown': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4975/230901958.py:153: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  portfolio_value_df['daily_return'] = portfolio_value_df['portfolio_value'].pct_change().fillna(0)\n",
      "/tmp/ipykernel_4975/230901958.py:155: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sharpe_ratio = (portfolio_value_df['daily_return'].mean() / portfolio_value_df['daily_return'].std()) * np.sqrt(252)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Placeholder for ticker data (train and test)\n",
    "ticker_list = ['AAPL', 'MSFT', 'NVDA', 'AMZN', 'TSLA', 'HD', \n",
    "               'UNH', 'JNJ', 'LLY', 'JPM', 'BAC', 'WFC', \n",
    "               'XOM', 'CVX', 'COP', 'PG', 'KO', 'PEP',\n",
    "               'BA', 'CAT', 'UPS', 'LIN', 'APD', 'SHW', \n",
    "               'PLD', 'AMT', 'CCI', 'NEE', 'DUK', 'SO']\n",
    "\n",
    "# Load the trained model\n",
    "best_model = joblib.load('best_xgb_model.pkl')\n",
    "\n",
    "# Initialize variables for backtesting\n",
    "initial_cash = 100000  # Initial capital\n",
    "cash = initial_cash\n",
    "portfolio = {ticker: 0 for ticker in ticker_list}  # Track positions in each stock\n",
    "\n",
    "# Initialize DataFrames\n",
    "trade_log = pd.DataFrame(columns=['ticker', 'date', 'position_type', 'trade_type', 'entry_price', 'exit_price', 'return', 'signal', 'pnl'])\n",
    "portfolio_value_df = pd.DataFrame(columns=['date', 'portfolio_value', 'cash_value', 'position_value'])\n",
    "\n",
    "# Load your price data for 2022 and 2023\n",
    "def get_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch data from PostgreSQL; if missing, download from Databento.\n",
    "    \"\"\"\n",
    "    df = dbs.get_data_from_postgresql(ticker, start_date, end_date)\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(f\"Data for {ticker} not found in PostgreSQL. Fetching from Databento.\")\n",
    "        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        df = dbs.get_data_from_databento(ticker, start_date, end_date)\n",
    "\n",
    "        dbs.upload_to_postgresql(df, ticker)\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['ts_event'])\n",
    "    df = df.sort_values(by='date')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Cache price data for all tickers\n",
    "data_cache = {ticker: get_data(ticker, '2023-01-01', '2023-12-31') for ticker in ticker_list}\n",
    "\n",
    "# Generate signals for each ticker\n",
    "signals = {}\n",
    "for ticker in ticker_list:\n",
    "    train_data = get_data(ticker, '2019-01-01', '2022-12-31')\n",
    "    test_data = get_data(ticker, '2023-01-01', '2023-12-31')\n",
    "\n",
    "    dtrain, y_train, X_train = prepdata(ticker, '2019-01-01', '2022-12-31')\n",
    "    dtest, y_test, X_test = prepdata(ticker, '2023-01-01', '2023-12-31')\n",
    "\n",
    "    # Ensure equal length for test data, X_test, and y_test\n",
    "    min_len = min(len(test_data), len(X_test), len(y_test))\n",
    "    test_data = test_data.iloc[:min_len]\n",
    "    X_test = X_test[:min_len]\n",
    "    y_test = y_test[:min_len]\n",
    "\n",
    "    print(f\"Test data shape: {test_data.shape}, X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    # Ensure equal length for test data and predictions\n",
    "    signals[ticker] = pd.DataFrame({'date': test_data['date'].values, 'predicted_change': y_pred})\n",
    "\n",
    "# Backtesting loop day by day\n",
    "unique_dates = sorted(set(signals[ticker_list[0]]['date']))\n",
    "for current_date in unique_dates:\n",
    "    position_value = 0\n",
    "    for ticker in ticker_list:\n",
    "        ticker_signal = signals[ticker]\n",
    "        signal_data = ticker_signal[ticker_signal['date'] == current_date]\n",
    "        if signal_data.empty:\n",
    "            continue\n",
    "\n",
    "        predicted_change = signal_data['predicted_change'].iloc[0]\n",
    "        current_day_data = data_cache[ticker][data_cache[ticker]['date'] == current_date]\n",
    "        if current_day_data.empty:\n",
    "            continue\n",
    "\n",
    "        next_open = current_day_data['open'].iloc[0]\n",
    "        next_close = current_day_data['close'].iloc[0]\n",
    "        signal = 'long' if predicted_change > 0 else 'short'\n",
    "\n",
    "        # Update position values if no sell signal\n",
    "        if portfolio[ticker] > 0 and predicted_change <= -0.01:\n",
    "            exit_price = next_close\n",
    "            entry_price = trade_log[trade_log['ticker'] == ticker]['entry_price'].iloc[-1]  # Get the last buy price\n",
    "            trade_return = (exit_price - entry_price) / entry_price\n",
    "            pnl = (exit_price - entry_price) * portfolio[ticker]\n",
    "\n",
    "            # Log sell trade\n",
    "            trade_log = pd.concat([trade_log, pd.DataFrame({\n",
    "                'ticker': [ticker],\n",
    "                'date': [current_date],\n",
    "                'position_type': ['long'],\n",
    "                'trade_type': ['sell'],\n",
    "                'entry_price': [entry_price],\n",
    "                'exit_price': [exit_price],\n",
    "                'return': [trade_return],\n",
    "                'signal': [signal],\n",
    "                'portfolio_value_start': [entry_price * portfolio[ticker]],\n",
    "                'portfolio_value_end': [exit_price * portfolio[ticker]],\n",
    "                'pnl': [pnl]\n",
    "            })], ignore_index=True)\n",
    "            cash += portfolio[ticker] * exit_price\n",
    "            portfolio[ticker] = 0\n",
    "\n",
    "        # Buy signal\n",
    "        if predicted_change > 0.01 and portfolio[ticker] == 0:\n",
    "            num_shares = cash // next_open  # Integer division for full shares\n",
    "            if num_shares > 0:\n",
    "                portfolio[ticker] = num_shares\n",
    "                cash -= num_shares * next_open\n",
    "\n",
    "                # Log buy trade\n",
    "                trade_log = pd.concat([trade_log, pd.DataFrame({\n",
    "                    'ticker': [ticker],\n",
    "                    'date': [current_date],\n",
    "                    'position_type': ['long'],\n",
    "                    'trade_type': ['buy'],\n",
    "                    'entry_price': [next_open],\n",
    "                    'exit_price': [np.nan],\n",
    "                    'return': [np.nan],\n",
    "                    'signal': [signal],\n",
    "                    'portfolio_value_start': [next_open * num_shares],\n",
    "                    'portfolio_value_end': [np.nan],\n",
    "                    'pnl': [np.nan]\n",
    "                })], ignore_index=True)\n",
    "\n",
    "        # Update position value\n",
    "        position_value += portfolio[ticker] * next_close\n",
    "\n",
    "    # Calculate portfolio value daily and append to rolling DataFrame\n",
    "    portfolio_value = position_value + cash\n",
    "    portfolio_value_df = pd.concat([portfolio_value_df, pd.DataFrame({'date': [current_date], 'portfolio_value': [portfolio_value], 'cash_value': [cash], 'position_value': [position_value]})], ignore_index=True)\n",
    "\n",
    "# Calculate final portfolio value\n",
    "final_portfolio_value = cash + sum([portfolio[ticker] * data_cache[ticker][data_cache[ticker]['date'] == unique_dates[-1]]['close'].iloc[0] for ticker in ticker_list if not data_cache[ticker][data_cache[ticker]['date'] == unique_dates[-1]].empty])\n",
    "\n",
    "print(f\"Final Portfolio Value: {final_portfolio_value}\")\n",
    "\n",
    "# Save portfolio value log to CSV\n",
    "portfolio_value_df.to_csv('portfolio_value_log.csv', index=False)\n",
    "\n",
    "# Calculate portfolio performance metrics\n",
    "def calculate_portfolio_metrics(portfolio_value_df):\n",
    "    # Calculate daily returns from portfolio value changes\n",
    "    portfolio_value_df['daily_return'] = portfolio_value_df['portfolio_value'].pct_change().fillna(0)\n",
    "\n",
    "    sharpe_ratio = (portfolio_value_df['daily_return'].mean() / portfolio_value_df['daily_return'].std()) * np.sqrt(252)\n",
    "    \n",
    "    downside_risk = portfolio_value_df[portfolio_value_df['daily_return'] < 0]['daily_return'].std()\n",
    "    sortino_ratio = (portfolio_value_df['daily_return'].mean() / downside_risk) * np.sqrt(252) if downside_risk > 0 else float('nan')\n",
    "\n",
    "    # Max drawdown calculation\n",
    "    cumulative_return = (1 + portfolio_value_df['daily_return']).cumprod()\n",
    "    drawdown = cumulative_return / cumulative_return.cummax() - 1\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    return {\n",
    "        'Sharpe Ratio': sharpe_ratio if portfolio_value_df['daily_return'].std() > 0 else float('nan'),\n",
    "        'Sortino Ratio': sortino_ratio,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "# Recalculate portfolio metrics using the rolling portfolio value\n",
    "portfolio_metrics = calculate_portfolio_metrics(portfolio_value_df)\n",
    "print(portfolio_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# as of 14OCT24, realized this is the wrong format for a file like this. Moved to backtest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_log.to_csv('trade_log.csv', index=False)\n",
    "# Need to add logic to close all positions at the end of the backtest\n",
    "# for each trade, note if stopped out as well\n",
    "# add a column for the trade duration\n",
    "# add a column for rolling portfolio value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ts_event  rtype  publisher_id  instrument_id   open  \\\n",
      "245 2023-12-22 00:00:00+00:00     35             2          14941  69.77   \n",
      "246 2023-12-26 00:00:00+00:00     35             2          14941  69.21   \n",
      "247 2023-12-27 00:00:00+00:00     35             2          14941  69.45   \n",
      "248 2023-12-28 00:00:00+00:00     35             2          14941  69.38   \n",
      "249 2023-12-29 00:00:00+00:00     35             2          14941  69.79   \n",
      "\n",
      "      high     low   close  volume symbol                      date  \n",
      "245  70.20  69.335  69.455  425671     SO 2023-12-22 00:00:00+00:00  \n",
      "246  69.83  69.170  69.450  259301     SO 2023-12-26 00:00:00+00:00  \n",
      "247  69.66  69.020  69.410  320898     SO 2023-12-27 00:00:00+00:00  \n",
      "248  70.32  69.280  70.170  371126     SO 2023-12-28 00:00:00+00:00  \n",
      "249  70.25  69.630  70.120  337896     SO 2023-12-29 00:00:00+00:00  \n"
     ]
    }
   ],
   "source": [
    "print(test_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.08836333e-03 -8.08836333e-03 -8.08836333e-03 -8.08836333e-03\n",
      " -8.08836333e-03 -8.08836333e-03 -8.08836333e-03 -8.08836333e-03\n",
      " -8.08836333e-03 -8.08836333e-03 -8.08836333e-03 -8.08836333e-03\n",
      " -9.07882676e-03 -1.90817919e-02 -5.11187408e-03  3.73533927e-03\n",
      "  3.55685683e-04 -8.94555170e-03 -5.88748325e-03  1.17391599e-02\n",
      "  2.22211075e-03 -2.06932873e-02 -5.61465509e-03 -9.84477112e-04\n",
      " -2.64076646e-02 -3.60762514e-02  7.78150978e-03  2.72616148e-02\n",
      " -5.05026430e-03 -4.02532285e-03 -1.61050837e-02  1.31400917e-02\n",
      " -1.87743288e-02 -1.94315538e-02  8.57295748e-03  1.30745291e-03\n",
      " -2.51794364e-02 -4.45407778e-02 -1.79926790e-02  1.94552541e-02\n",
      "  1.79682877e-02  9.63837747e-03 -7.19479565e-03 -6.01667538e-03\n",
      "  8.08004756e-03 -6.08328078e-03  7.62430765e-03  1.75425969e-02\n",
      "  1.88987870e-02  9.87434201e-03 -5.49496571e-03  1.21233100e-03\n",
      " -1.46229174e-02 -7.40941241e-03 -4.62152064e-03  1.74669661e-02\n",
      "  2.82894005e-03  3.58172273e-03  1.19408509e-02  4.42109304e-04\n",
      "  5.19943936e-03 -2.10564188e-03  4.24681371e-03  1.70089100e-02\n",
      "  1.70201883e-02  8.05539021e-04  7.29701540e-04  8.17162544e-03\n",
      "  9.09207668e-03 -4.29161685e-03  2.11847294e-03  1.41147403e-02\n",
      "  5.72652835e-03 -2.94154696e-03  9.29834414e-03  1.39767900e-02\n",
      "  2.84915511e-03 -2.21682359e-02  3.95817729e-03  1.10241594e-02\n",
      "  1.02836555e-02 -1.29677812e-02 -8.00568052e-03  1.53052546e-02\n",
      "  1.08368676e-02 -4.26697964e-03  5.58243599e-03  3.48236645e-03\n",
      " -1.20896855e-02 -2.13479884e-02 -1.89261660e-02 -1.96896717e-02\n",
      " -5.03347442e-03  1.24247817e-04 -5.68473339e-03 -5.78413252e-03\n",
      " -7.61728967e-03 -4.43995371e-03 -1.42350821e-02 -1.23784747e-02\n",
      " -7.96559919e-03 -2.61844543e-04 -1.35904439e-02 -5.01932483e-03\n",
      "  2.02241857e-02 -1.61793636e-04  1.43516045e-02  1.61606241e-02\n",
      "  1.10500343e-02 -7.53852725e-03 -1.24506978e-02 -7.47159123e-03\n",
      "  7.30523793e-03  1.71394851e-02 -2.57538203e-02  1.12487460e-02\n",
      "  5.55297220e-03 -2.16098130e-02  1.03652067e-02 -1.24971394e-03\n",
      " -4.05351557e-02 -1.23557122e-02  2.97556408e-02  1.19260726e-02\n",
      "  2.08990127e-02 -3.74139822e-03 -6.85206661e-03 -1.89111717e-02\n",
      "  9.22361948e-03  1.88054908e-02  6.31152373e-03 -4.39275568e-03\n",
      " -2.65105627e-02 -3.18993665e-02  9.09465365e-03  2.12132931e-02\n",
      "  1.72241554e-02  6.90086139e-03 -6.50262227e-03 -4.37072478e-04\n",
      "  1.89970888e-04 -8.13705660e-03 -3.28383734e-03 -1.02240359e-02\n",
      " -1.28611727e-02 -2.52398085e-02 -2.65773982e-02 -1.36294484e-03\n",
      "  7.01489905e-03  7.64219696e-03  5.92107978e-03  5.85977745e-04\n",
      " -8.56481306e-03 -1.86295975e-02 -4.43283003e-03  3.92888673e-03\n",
      " -8.67978018e-03 -3.35321343e-03  3.65169463e-03  1.64011406e-04\n",
      "  1.75437587e-03  1.59479063e-02 -4.08300338e-03  8.84545129e-03\n",
      "  1.21494299e-02 -1.77642033e-02 -6.42242143e-03 -7.81056238e-03\n",
      " -4.06150753e-03  1.77114811e-02  9.51756164e-03  4.42151772e-03\n",
      "  1.96156353e-02  1.52074853e-02  2.91154161e-02  1.42282993e-02\n",
      "  5.40967612e-03  7.68836029e-03  1.79611426e-03 -1.62638705e-02\n",
      " -1.10863643e-02 -2.68970220e-03 -3.15094255e-02 -2.96150800e-02\n",
      " -1.94789618e-02 -1.71913058e-02 -2.39756089e-02  1.20691722e-02\n",
      "  1.60899125e-02 -1.67497550e-04  5.99048362e-05  6.43041125e-03\n",
      "  1.75649452e-03  1.19696865e-02 -3.73631925e-03  8.08569312e-04\n",
      "  1.88486446e-02  8.57938081e-03 -1.21104838e-02 -4.42576734e-03\n",
      " -1.10276137e-02 -1.24902111e-02  2.37129373e-03  6.84962841e-03\n",
      "  5.36033185e-03 -5.95493522e-03 -7.26983091e-03  9.96550359e-03\n",
      "  2.23647524e-02 -9.99165000e-04  9.52254701e-03 -9.83488280e-03\n",
      " -1.22277765e-02 -4.71607503e-03 -1.43865438e-03 -8.05966742e-03\n",
      " -1.20070735e-02  1.31968083e-02  2.99326400e-03  1.29970862e-02\n",
      " -4.94931592e-05 -8.08489509e-03  1.51607406e-03  1.37275062e-03\n",
      "  4.64890013e-03  1.58348028e-02  3.56517383e-03 -1.00992052e-02\n",
      "  1.04678851e-02  1.79204270e-02 -6.15882361e-03 -1.24596106e-02\n",
      "  2.31518410e-02 -2.16500391e-03 -8.09443835e-03 -1.31424831e-03\n",
      " -1.39409443e-02  2.34252121e-02 -6.26490265e-03 -2.65650153e-02\n",
      "  3.72595829e-03  3.05662490e-03 -1.81863662e-02 -6.74675498e-03\n",
      "  7.97365047e-03  1.31009030e-03 -4.36388596e-04  1.22911818e-02\n",
      "  6.95531396e-03]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         date   open  close\n",
      "247 2023-12-27 00:00:00+00:00  69.45  69.41\n",
      "248 2023-12-28 00:00:00+00:00  69.38  70.17\n",
      "249 2023-12-29 00:00:00+00:00  69.79  70.12\n"
     ]
    }
   ],
   "source": [
    "print(test_data[['date', 'open', 'close']].iloc[i:i+5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (249, 74), y_test shape: (249,)\n"
     ]
    }
   ],
   "source": [
    "dtest, y_test, X_test = prepdata(ticker, '2023-01-01', '2023-12-31')\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         date   open   close\n",
      "245 2023-12-22 00:00:00+00:00  69.77  69.455\n",
      "246 2023-12-26 00:00:00+00:00  69.21  69.450\n",
      "247 2023-12-27 00:00:00+00:00  69.45  69.410\n",
      "248 2023-12-28 00:00:00+00:00  69.38  70.170\n",
      "249 2023-12-29 00:00:00+00:00  69.79  70.120\n"
     ]
    }
   ],
   "source": [
    "print(test_data[['date', 'open', 'close']].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date   2023-01-03 00:00:00+00:00\n",
      "dtype: datetime64[ns, UTC] date   2023-12-29 00:00:00+00:00\n",
      "dtype: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[['date']].min(), test_data[['date']].max())  # Check the date range before conversion to DMatrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Combine training data for multiple tickers\n",
    "tickers = ticker_list #tickerlist defined at the top of file\n",
    "X_combined = []\n",
    "y_combined = []\n",
    "\n",
    "# Loop through tickers with progress bar\n",
    "for ticker in tqdm(tickers, desc=\"Processing tickers\"):\n",
    "    dtrain, y_train, X_train = prepdata(ticker, '2019-01-01', '2022-12-31')\n",
    "    X_combined.append(X_train)\n",
    "    y_combined.append(y_train)\n",
    "\n",
    "X_combined = pd.concat(X_combined)\n",
    "y_combined = np.concatenate(y_combined)\n",
    "\n",
    "# Initialize model and parameter grid\n",
    "#xgb_regressor = XGBRegressor()\n",
    "xgb_regressor = XGBRegressor(tree_method=\"hist\", device=\"cuda\") #GPu accelerated\n",
    "\n",
    "#xgb_regressor.set_params(predictor=\"gpu_predictor\") #GPu accelerated\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0.1, 1, 10],\n",
    "    'lambda': [0.1, 1, 10],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for time-based cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Perform GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=tscv, scoring='r2', verbose=1)\n",
    "grid_search.fit(X_combined, y_combined)\n",
    "\n",
    "# Print the best parameter  s and R² score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best R² Score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 30/30 [00:03<00:00,  8.43it/s]\n",
      "GridSearch Progress:   0%|          | 0/131220 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 26244 candidates, totalling 131220 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearch Progress:   0%|          | 0/131220 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Continue training\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Fit the model using CuPy data\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_combined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTQDMCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Print the best parameters and R² score\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:881\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    878\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    880\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m--> 881\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/utils/metaestimators.py:159\u001b[0m, in \u001b[0;36m_safe_split\u001b[0;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[1;32m    156\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m _safe_indexing(X, indices)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/sklearn/utils/_indexing.py:176\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_indexing\u001b[39m(X, indices, \u001b[38;5;241m*\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return rows, items or columns of X using indices.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    .. warning::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    array([1, 3, 5])\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Combine training data for multiple tickers\n",
    "tickers = ticker_list  # Define ticker_list at the top of your file\n",
    "X_combined = []\n",
    "y_combined = []\n",
    "\n",
    "# Loop through tickers with a progress bar\n",
    "for ticker in tqdm(tickers, desc=\"Processing tickers\"):\n",
    "    dtrain, y_train, X_train = prepdata(ticker, '2019-01-01', '2022-12-31')  # Assumes prepdata is defined\n",
    "    # Convert to NumPy, then to CuPy arrays\n",
    "    X_combined.append(cp.array(X_train.to_numpy()))  \n",
    "    y_combined.append(cp.array(y_train.to_numpy()))\n",
    "\n",
    "# Concatenate CuPy arrays\n",
    "X_combined = cp.concatenate(X_combined)\n",
    "y_combined = cp.concatenate(y_combined)\n",
    "\n",
    "# Initialize model and parameter grid with GPU support using 'device' parameter\n",
    "xgb_regressor = XGBRegressor(tree_method=\"hist\", device=\"cuda\")  # Updated for GPU support\n",
    "\n",
    "# Set parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0.1, 1, 10],\n",
    "    'lambda': [0.1, 1, 10],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for time-based cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Convert CuPy arrays to NumPy arrays for scikit-learn compatibility\n",
    "X_combined_np = cp.asnumpy(X_combined)\n",
    "y_combined_np = cp.asnumpy(y_combined)\n",
    "\n",
    "# Perform GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=tscv, scoring='r2', verbose=1)\n",
    "grid_search.fit(X_combined_np, y_combined_np)\n",
    "\n",
    "# Print the best parameters and R² score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best R² Score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tickers: 100%|██████████| 30/30 [00:03<00:00,  8.21it/s]\n",
      "XGBoost CV Progress: 100%|██████████| 8748/8748 [48:46<00:00,  2.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 9, 'subsample': 1.0, 'colsample_bytree': 1.0, 'alpha': 0.1, 'lambda': 0.1, 'gamma': 0}\n",
      "Best R² Score: 0.9469016867570385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#cupy v2 with progress bar\n",
    "import cupy as cp\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Combine training data for multiple tickers\n",
    "tickers = ticker_list  # Define ticker_list at the top of your file\n",
    "X_combined = []\n",
    "y_combined = []\n",
    "\n",
    "# Loop through tickers with a progress bar\n",
    "for ticker in tqdm(tickers, desc=\"Processing tickers\"):\n",
    "    dtrain, y_train, X_train = prepdata(ticker, '2019-01-01', '2022-12-31')  # Assumes prepdata is defined\n",
    "    # Convert to CuPy arrays directly\n",
    "    X_combined.append(cp.array(X_train.to_numpy()))  \n",
    "    y_combined.append(cp.array(y_train.to_numpy()))\n",
    "\n",
    "# Concatenate CuPy arrays\n",
    "X_combined = cp.concatenate(X_combined)\n",
    "y_combined = cp.concatenate(y_combined)\n",
    "\n",
    "# Initialize model with GPU support using the 'device' parameter\n",
    "xgb_regressor = XGBRegressor(tree_method=\"hist\", device=\"cuda\")\n",
    "\n",
    "# Set parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0.1, 1, 10],\n",
    "    'lambda': [0.1, 1, 10],\n",
    "    'gamma': [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for time-based cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Perform cross-validation using custom progress bar\n",
    "total_fits = len(param_grid['learning_rate']) * len(param_grid['max_depth']) * len(param_grid['n_estimators']) \\\n",
    "    * len(param_grid['subsample']) * len(param_grid['colsample_bytree']) * len(param_grid['alpha']) \\\n",
    "    * len(param_grid['lambda']) * len(param_grid['gamma'])\n",
    "\n",
    "with tqdm(total=total_fits, desc=\"XGBoost CV Progress\") as pbar:\n",
    "    best_score = float('-inf')\n",
    "    best_params = None\n",
    "\n",
    "    # Loop through the parameter grid\n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for n_estimators in param_grid['n_estimators']:\n",
    "                for subsample in param_grid['subsample']:\n",
    "                    for colsample_bytree in param_grid['colsample_bytree']:\n",
    "                        for alpha in param_grid['alpha']:\n",
    "                            for lambd in param_grid['lambda']:\n",
    "                                for gamma in param_grid['gamma']:\n",
    "                                        # Define current parameter set\n",
    "                                        params = {\n",
    "                                            'learning_rate': learning_rate,\n",
    "                                            'max_depth': max_depth,\n",
    "                                            'subsample': subsample,\n",
    "                                            'colsample_bytree': colsample_bytree,\n",
    "                                            'alpha': alpha,\n",
    "                                            'lambda': lambd,\n",
    "                                            'gamma': gamma\n",
    "                                        }\n",
    "\n",
    "                                        # Apply parameters to the regressor\n",
    "                                        xgb_regressor.set_params(**params)\n",
    "\n",
    "                                        # Perform cross-validation using GPU\n",
    "                                        xgb_regressor.fit(cp.asnumpy(X_combined), cp.asnumpy(y_combined))\n",
    "\n",
    "                                        # Get score (you can adjust the scoring metric based on your use case)\n",
    "                                        score = xgb_regressor.score(cp.asnumpy(X_combined), cp.asnumpy(y_combined))\n",
    "\n",
    "                                        if score > best_score:\n",
    "                                            best_score = score\n",
    "                                            best_params = params\n",
    "\n",
    "                                        pbar.update(1)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best R² Score: {best_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CUDF and tqdm callbacks for accelerated gpu processing and progress tracking\n",
    "\n",
    "import cudf\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Combine training data for multiple tickers\n",
    "tickers = ticker_list  # Define ticker_list at the top of your file\n",
    "X_combined = []\n",
    "y_combined = []\n",
    "\n",
    "# Loop through tickers with a progress bar\n",
    "for ticker in tqdm(tickers, desc=\"Processing tickers\"):\n",
    "    dtrain, y_train, X_train = prepdata(ticker, '2019-01-01', '2022-12-31')  # Assumes prepdata is defined\n",
    "    # Convert pandas DataFrame to cudf DataFrame\n",
    "    X_combined.append(cudf.DataFrame.from_pandas(X_train))\n",
    "    y_combined.append(cudf.Series.from_pandas(y_train))\n",
    "\n",
    "# Concatenate cudf DataFrames\n",
    "X_combined = cudf.concat(X_combined)\n",
    "y_combined = cudf.concat(y_combined)\n",
    "\n",
    "# Initialize model and parameter grid with GPU support using 'device' parameter\n",
    "xgb_regressor = xgb.XGBRegressor(tree_method=\"hist\", device=\"cuda\")  # Updated for GPU support\n",
    "\n",
    "# Set parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0.1, 1, 10],\n",
    "    'lambda': [0.1, 1, 10],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for time-based cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Convert cudf DataFrame to NumPy arrays for compatibility with scikit-learn\n",
    "X_combined_np = X_combined.to_pandas().to_numpy()\n",
    "y_combined_np = y_combined.to_pandas().to_numpy()\n",
    "\n",
    "# Add a progress bar for GridSearchCV fits\n",
    "n_candidates = len(param_grid['learning_rate']) * len(param_grid['max_depth']) * len(param_grid['n_estimators']) \\\n",
    "    * len(param_grid['subsample']) * len(param_grid['colsample_bytree']) * len(param_grid['alpha']) \\\n",
    "    * len(param_grid['lambda']) * len(param_grid['gamma']) * len(param_grid['min_child_weight'])\n",
    "\n",
    "with tqdm(total=n_candidates * 5, desc=\"GridSearch Progress\") as pbar:  # 5 = number of folds in TimeSeriesSplit\n",
    "    class TQDMCallback(xgb.callback.TrainingCallback):\n",
    "        def after_iteration(self, model, epoch, evals_log):\n",
    "            pbar.update(1)\n",
    "            return False  # Continue training\n",
    "\n",
    "    # Perform GridSearchCV with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=tscv, scoring='r2', verbose=1)\n",
    "    grid_search.fit(X_combined_np, y_combined_np, callbacks=[TQDMCallback()])\n",
    "\n",
    "# Print the best parameters and R² score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best R² Score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import cupy as cp\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def perform_grid_search(X, y, param_grid, model):\n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    best_score = float('-inf')\n",
    "    best_params = None\n",
    "    \n",
    "    with tqdm(total=len(param_combinations), desc=\"XGBoost CV Progress\") as pbar:\n",
    "        for param_set in param_combinations:\n",
    "            params = dict(zip(param_grid.keys(), param_set))\n",
    "            model.set_params(**params)\n",
    "            \n",
    "            model.fit(cp.asnumpy(X), cp.asnumpy(y))\n",
    "            score = model.score(cp.asnumpy(X), cp.asnumpy(y))\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Example usage:\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0.1, 1, 10],\n",
    "    'lambda': [0.1, 1, 10],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Perform the search\n",
    "best_params, best_score = perform_grid_search(X_combined, y_combined, param_grid, xgb_regressor)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best R² Score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_xgb_model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After gridsearchiing using one of the above snippets:\n",
    "# Create a new model using the best parameters\n",
    "best_xgb_model = xgb.XGBRegressor(**best_params, tree_method=\"hist\", device=\"cuda\")\n",
    "\n",
    "# Train the model using the training data\n",
    "best_xgb_model.fit(X_combined_np, y_combined_np)\n",
    "\n",
    "# Optionally, save the trained model to a file\n",
    "import joblib\n",
    "joblib.dump(best_xgb_model, 'best_xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_best_model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming your model is the best one after grid search\n",
    "best_model = xgb_regressor\n",
    "\n",
    "# Save the best model\n",
    "#best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, 'xgb_best_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "best_model = joblib.load('xgb_best_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on AAPL:\n",
      "MAE: 0.006129980545684062\n",
      "MSE: 6.707176819986686e-05\n",
      "R²: 0.9758889986845886\n",
      "---\n",
      "Performance on MSFT:\n",
      "MAE: 0.005769208710626976\n",
      "MSE: 6.284043404517403e-05\n",
      "R²: 0.8467398029675239\n",
      "---\n",
      "Performance on NVDA:\n",
      "MAE: 0.008611918454611345\n",
      "MSE: 0.0001288901994853895\n",
      "R²: 0.9644950916256266\n",
      "---\n",
      "Performance on AMZN:\n",
      "MAE: 0.006882658121104334\n",
      "MSE: 9.130347443430085e-05\n",
      "R²: 0.992079503025956\n",
      "---\n",
      "Performance on TSLA:\n",
      "MAE: 0.010956372819544449\n",
      "MSE: 0.0002138563780594483\n",
      "R²: 0.9660448994913136\n",
      "---\n",
      "Performance on HD:\n",
      "MAE: 0.005488003530302216\n",
      "MSE: 5.5421046097711826e-05\n",
      "R²: 0.8608378271165196\n",
      "---\n",
      "Performance on UNH:\n",
      "MAE: 0.005365407206637782\n",
      "MSE: 5.760203111589428e-05\n",
      "R²: 0.8454586279588304\n",
      "---\n",
      "Performance on JNJ:\n",
      "MAE: 0.004026033129866135\n",
      "MSE: 2.9471292142832132e-05\n",
      "R²: 0.84006134424163\n",
      "---\n",
      "Performance on LLY:\n",
      "MAE: 0.005684191315116926\n",
      "MSE: 6.226316987209641e-05\n",
      "R²: 0.8622775402991097\n",
      "---\n",
      "Performance on JPM:\n",
      "MAE: 0.005824185272330074\n",
      "MSE: 6.48761433455416e-05\n",
      "R²: 0.8622051216597152\n",
      "---\n",
      "Performance on BAC:\n",
      "MAE: 0.006464277229117672\n",
      "MSE: 7.83654733251753e-05\n",
      "R²: 0.8641289044367654\n",
      "---\n",
      "Performance on WFC:\n",
      "MAE: 0.006983614612125252\n",
      "MSE: 8.943539559516004e-05\n",
      "R²: 0.8752054537704014\n",
      "---\n",
      "Performance on XOM:\n",
      "MAE: 0.0070432682343422445\n",
      "MSE: 8.475878087718662e-05\n",
      "R²: 0.8568752284388993\n",
      "---\n",
      "Performance on CVX:\n",
      "MAE: 0.006300786950881856\n",
      "MSE: 7.124416156379717e-05\n",
      "R²: 0.8835413773861943\n",
      "---\n",
      "Performance on COP:\n",
      "MAE: 0.008279843240922388\n",
      "MSE: 0.00011988315558013183\n",
      "R²: 0.8767503136112942\n",
      "---\n",
      "Performance on PG:\n",
      "MAE: 0.004277848500912792\n",
      "MSE: 3.573713048655569e-05\n",
      "R²: 0.8393690050601184\n",
      "---\n",
      "Performance on KO:\n",
      "MAE: 0.0041524026958702645\n",
      "MSE: 3.347218013667845e-05\n",
      "R²: 0.8275085074378687\n",
      "---\n",
      "Performance on PEP:\n",
      "MAE: 0.004173639687247209\n",
      "MSE: 3.2460322443024596e-05\n",
      "R²: 0.8520755256138032\n",
      "---\n",
      "Performance on BA:\n",
      "MAE: 0.00805884444859757\n",
      "MSE: 0.0001201609496872891\n",
      "R²: 0.9131658344208223\n",
      "---\n",
      "Performance on CAT:\n",
      "MAE: 0.006348288761370472\n",
      "MSE: 7.216345356255406e-05\n",
      "R²: 0.8473511182768003\n",
      "---\n",
      "Performance on UPS:\n",
      "MAE: 0.005824064093592729\n",
      "MSE: 6.32548630022229e-05\n",
      "R²: 0.8509443544778372\n",
      "---\n",
      "Performance on LIN:\n",
      "MAE: 0.005388360380633341\n",
      "MSE: 5.5023247294227066e-05\n",
      "R²: 0.8431045728749711\n",
      "---\n",
      "Performance on APD:\n",
      "MAE: 0.005519341147448948\n",
      "MSE: 5.931436185931229e-05\n",
      "R²: 0.8599159273739807\n",
      "---\n",
      "Performance on SHW:\n",
      "MAE: 0.005698320493019794\n",
      "MSE: 6.360553781458262e-05\n",
      "R²: 0.9665469006426306\n",
      "---\n",
      "Performance on PLD:\n",
      "MAE: 0.005830834087476463\n",
      "MSE: 6.346947291923913e-05\n",
      "R²: 0.869836458360308\n",
      "---\n",
      "Performance on AMT:\n",
      "MAE: 0.005515328898271705\n",
      "MSE: 5.733575859674479e-05\n",
      "R²: 0.8594409883852806\n",
      "---\n",
      "Performance on CCI:\n",
      "MAE: 0.005455432340455675\n",
      "MSE: 5.7419113843499515e-05\n",
      "R²: 0.8592890038151768\n",
      "---\n",
      "Performance on NEE:\n",
      "MAE: 0.005613279796789196\n",
      "MSE: 5.747324717373575e-05\n",
      "R²: 0.9793498242332321\n",
      "---\n",
      "Performance on DUK:\n",
      "MAE: 0.004699812537138399\n",
      "MSE: 4.2390359689047575e-05\n",
      "R²: 0.8489866929052277\n",
      "---\n",
      "Performance on SO:\n",
      "MAE: 0.005021805598237173\n",
      "MSE: 5.1674726430152475e-05\n",
      "R²: 0.8462657738092296\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Test the model on new tickers\n",
    "\n",
    "# Load the saved model\n",
    "best_model = joblib.load('best_xgb_model.pkl')\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    # Prepare data for the new ticker\n",
    "    dtrain, y_train, X_train = prepdata(ticker, '2019-01-01', '2022-12-31')\n",
    "    \n",
    "    # Use the saved model to make predictions\n",
    "    y_pred = best_model.predict(X_train)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    mae = mean_absolute_error(y_train, y_pred)\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    r2 = r2_score(y_train, y_pred)\n",
    "    \n",
    "    print(f\"Performance on {ticker}:\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"R²: {r2}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp methods for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtester get_data function\n",
    "from datetime import datetime\n",
    "# Load your price data for 2022 and 2023\n",
    "def get_data(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch data from PostgreSQL; if missing, download from Databento.\n",
    "    \"\"\"\n",
    "    # Check if data already exists in PostgreSQL\n",
    "    df = dbs.get_data_from_postgresql(ticker, start_date, end_date)  # Assuming this method exists in databento_sql.py\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        # If data is missing, download from Databento\n",
    "        print(f\"Data for {ticker} not found in PostgreSQL. Fetching from Databento.\")\n",
    "        # Convert start_date and end_date to datetime objects\n",
    "        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        df = dbs.get_data_from_databento(ticker, start_date, end_date)  # Download from Databento and return DataFrame\n",
    "\n",
    "        # Upload the data to PostgreSQL for future use\n",
    "        dbs.upload_to_postgresql(df, ticker)\n",
    "\n",
    "    # Ensure the 'date' column is datetime and sorted by date\n",
    "    df['date'] = pd.to_datetime(df['ts_event'])\n",
    "    df = df.sort_values(by='date')\n",
    "\n",
    "    return df\n",
    "\n",
    "train_data = get_data('AAPL', '2019-01-01', '2022-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def get_data_from_postgresql(ticker, start_date=None, end_date=None, schema='databento_ohlcv'):\n",
    "    \"\"\"\n",
    "    Retrieves data for a given ticker from PostgreSQL database, optionally within a date range.\n",
    "    \"\"\"\n",
    "    # Fetch credentials from environment variables\n",
    "    pguser = os.getenv('pguser')\n",
    "    pgpass = os.getenv('pgpass')\n",
    "    pghost = os.getenv('pghost')\n",
    "\n",
    "    # Database connection URL using environment variables\n",
    "    db_url = f'postgresql://{pguser}:{pgpass}@{pghost}/FinancialData'\n",
    "    engine = create_engine(db_url)\n",
    "\n",
    "    try:\n",
    "        # Build the query\n",
    "        query = f'SELECT * FROM \"{schema}\".\"{ticker}\"'\n",
    "        if start_date is not None and end_date is not None:\n",
    "            # Ensure start_date and end_date are strings\n",
    "\n",
    "            # Convert date string to datetime object (if further handling is needed)\n",
    "            start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "            end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "            query += f\" WHERE ts_event BETWEEN '{start_date.strftime('%Y-%m-%d')}' AND '{end_date.strftime('%Y-%m-%d')}'\"\n",
    "\n",
    "        df = pd.read_sql(query, con=engine)\n",
    "\n",
    "        # Ensure ts_event is parsed as timezone-aware datetime\n",
    "        df['ts_event'] = pd.to_datetime(df['ts_event'], utc=True)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for {ticker} from PostgreSQL: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "train_data = get_data_from_postgresql('AAPL', '2019-01-01', '2022-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
